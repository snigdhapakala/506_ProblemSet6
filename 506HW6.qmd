---
title: "506ProblemSet6"
author: "Snigdha Pakala"
format:
  html:
    embed-resources: true
editor: visual
---

## Link to my GitHub repository: <https://github.com/snigdhapakala/506_ProblemSet6>

### Part a: Estimation of standard deviation

Calculate the average RF for each team in the Fielding table:

```{r}
library(DBI)     
library(dplyr)

lahman <- dbConnect(RSQLite::SQLite(), "lahman_1871-2022.sqlite")
lahman

dbListTables(lahman)

# Make my own table of Fielding data to work with
fielding <- dbReadTable(lahman, "Fielding")

fielding_df <- as.data.frame(fielding)

colnames(fielding)

# Calculate RF based on formula from assignment. Make sure if any NAs or if denominator is 0, to put that down as NA
fielding_df <- fielding_df %>%
  mutate(RF = ifelse(is.na(PO) | is.na(A) | is.na(InnOuts) | InnOuts == 0, NA, 3*((PO + A) / InnOuts)))

# Compute average RF per team, ignoring rows with NA RF
avg_rf_by_team <- fielding_df %>%
  group_by(teamID) %>%
  summarize(avg_RF = mean(RF, na.rm = TRUE)) %>%
  ungroup()


# Provide average RF per team
avg_rf_by_team <- fielding_df %>%
                  group_by(teamID) %>%
                  summarize(avg_RF = mean(RF, na.rm = TRUE)) %>%
                  ungroup() %>%
                  arrange(desc(avg_RF))

colnames(avg_rf_by_team) <- c("teamID", "avg_RF")

# Display top 10 results
head(avg_rf_by_team, 10)
```

#1: Estimate standard deviation without any parallel processing:

```{r}
# Create a matrix for different teams
boot_mat <- matrix(NA, nrow = 1000, ncol = length(unique(fielding_df$teamID)))

teams <- unique(fielding_df$teamID)

colnames(boot_mat) <- teams

set.seed(123)
# bootstrap samples
for (team in teams) {
  # Extract RF values for each team team
  team_specific_rf <- fielding_df[fielding_df$teamID == team, "RF"]
  
  # Perform bootstrap sampling and compute the mean of resampled RF
  boot_samples <- replicate(1000, mean(sample(team_specific_rf, 
                                              replace = TRUE), na.rm = TRUE))
  
  boot_mat[, team] <- boot_samples
}
# quick check: 
# head(boot_mat)

# Calculate estimated standard deviation per team
std_dev_df <- as.data.frame(apply(boot_mat, 2, sd, na.rm = TRUE))
colnames(std_dev_df) <- c("std_dev")
std_dev_df$teamID <- rownames(std_dev_df)

# Display results for standard deviation of RF per team - just 10 for computation purposes
top_10_std_dev <- avg_rf_by_team %>%
  inner_join(std_dev_df, by = "teamID") %>%
  arrange(desc(avg_RF)) %>%    # Arrange by descending AvgRF
  slice(1:10) 

top_10_std_dev
```

#2: Estimate standard deviation using parallel processing with the parallel package:

```{r}
library(parallel)
set.seed(123)

num_cores <- detectCores() - 1

#' Use this function for the bootstrap sampling with replicate
#'
#' @param team specific team within all of them
#'
#' @return bootstrap sample
#' @export
#'
#' @examples
bootstrap_function <- function(team) {
  # Get the RF values per team
  team_specific_rf <- fielding_df[fielding_df$teamID == team, "RF"]
  # 1000 bootstrap samples which calculate means 
  replicate(1000, mean(sample(team_specific_rf, replace = TRUE), na.rm = TRUE))
}

# Used 506 notes for mclapply and do.call
boot_mat_list <- mclapply(teams, bootstrap_function, mc.cores = num_cores)

boot_mat_parallel <- do.call(cbind, boot_mat_list)

colnames(boot_mat_parallel) <- teams

# Calculate standard deviation for the bootstrapping with parallel processing
std_dev_parallel_df <- as.data.frame(apply(boot_mat_parallel, 2, sd, na.rm = TRUE))

colnames(std_dev_parallel_df) <- c("std_dev_parallel")

std_dev_parallel_df$teamID <- rownames(std_dev_parallel_df)

# Display results for standard deviation of RF per team - just 10 for computation purposes
top_10_std_dev_parallel <- avg_rf_by_team %>%
  inner_join(std_dev_parallel_df, by = "teamID") %>%
  arrange(desc(avg_RF)) %>%    # Arrange by descending AvgRF
  slice(1:10) 

top_10_std_dev_parallel
```

#3: Estimate standard deviation using parallel processing with the future package:

```{r}
library(future)
set.seed(123)
options(future.seed = TRUE)
options(future.rng.onMisuse = "ignore")
plan(multisession, workers = detectCores() - 1)

#' Perform bootstrapping sample per team using replicate
#'
#' @param team specific team within all of them
#'
#' @return bootstrap sample
#' @export
#'
#' @examples
bootstrap_function <- function(team_data) {
  # 1000 bootstrap samples and the means calculated
  replicate(1000, mean(sample(team_data, replace = TRUE), na.rm = TRUE))
}

team_data_list <- lapply(teams, function(team) {
  fielding_df[fielding_df$teamID == team, "RF"]
})

# Make parallel jobs using futures
futures_list <- lapply(team_data_list, function(team_data) {
  future({
    bootstrap_function(team_data)
  })
})

# Get results using 506 notes
boot_mat_list <- lapply(futures_list, value)

boot_mat_future <- do.call(cbind, boot_mat_list)

colnames(boot_mat_future) <- teams

# Calculate standard deviation for the bootstrapping with future
std_dev_future_df <- as.data.frame(apply(boot_mat_future, 2, sd, na.rm = TRUE))

colnames(std_dev_future_df) <- c("std_dev_future")

std_dev_future_df$teamID <- rownames(std_dev_future_df)


# Display results for standard deviation of RF per team - just 10 for computation purposes
top_10_std_dev_future <- avg_rf_by_team %>%
  inner_join(std_dev_future_df, by = "teamID") %>%
  arrange(desc(avg_RF)) %>%    # Arrange by descending AvgRF
  slice(1:10) 

top_10_std_dev_future
```

### Part b: Generate a table showing the estimated RF and associated standard errors *for the teams with the 10 highest RF* from the three approaches.

```{r}

# Filter out NaN or Inf values from each standard deviation data frame
std_dev_df_clean <- std_dev_df %>%
  filter(!is.nan(std_dev) & !is.infinite(std_dev))

std_dev_parallel_df_clean <- std_dev_parallel_df %>%
  filter(!is.nan(std_dev_parallel) & !is.infinite(std_dev_parallel))

std_dev_future_df_clean <- std_dev_future_df %>%
  filter(!is.nan(std_dev_future) & !is.infinite(std_dev_future))

# Add average RF to each standard deviation table (cleaned)
std_dev_df_clean$avg_RF <- avg_rf_by_team$avg_RF[match(std_dev_df_clean$teamID, avg_rf_by_team$teamID)]

std_dev_parallel_df_clean$avg_RF <- avg_rf_by_team$avg_RF[match(std_dev_parallel_df_clean$teamID, avg_rf_by_team$teamID)]

std_dev_future_df_clean$avg_RF <- avg_rf_by_team$avg_RF[match(std_dev_future_df_clean$teamID, avg_rf_by_team$teamID)]

# Extract top 10 teams by avg_RF (after cleaning)
top_10_teams_clean <- avg_rf_by_team %>%
  filter(!is.na(avg_RF) & !is.infinite(avg_RF)) %>%
  arrange(desc(avg_RF)) %>%
  head(10) %>%
  pull(teamID)

# Filter for top 10 teams only, from the cleaned data frames

top_10_not_parallel_clean <- std_dev_df_clean %>%
  filter(teamID %in% top_10_teams_clean) %>%
  select(teamID, avg_RF, std_dev) %>%
  mutate(approach = "Not Parallel")

top_10_parallel_clean <- std_dev_parallel_df_clean %>%
  filter(teamID %in% top_10_teams_clean) %>%
  select(teamID, avg_RF, std_dev = std_dev_parallel) %>% 
  mutate(approach = "Parallel")

top_10_future_clean <- std_dev_future_df_clean %>%
  filter(teamID %in% top_10_teams_clean) %>%
  select(teamID, avg_RF, std_dev = std_dev_future) %>% 
  mutate(approach = "Future")

# Combine all results into a final table and sort
combined_table_clean <- bind_rows(
  top_10_not_parallel_clean,
  top_10_parallel_clean,
  top_10_future_clean
)

# Rename columns for clarity
colnames(combined_table_clean) <- c("Team ID", "Average RF", "Standard Error", "Approach")

# Sort the combined table 
final_table_clean <- combined_table_clean %>%
  arrange(Approach, desc(`Average RF`))

# Display the final table (top 10 teams only)
print(final_table_clean)


```

### Part c: Report and discuss the performance difference between the versions.

```{r}
##########################################################################################

# Timing non-parallel method
start_time <- Sys.time()

# Run the non-parallel bootstrap process here
boot_mat <- matrix(NA, nrow = 1000, ncol = length(unique(fielding_df$teamID)))
teams <- unique(fielding_df$teamID)
colnames(boot_mat) <- teams

set.seed(123)

# bootstrap samples
for (team in teams) {
  # Extract RF values for each team
  team_specific_rf <- fielding_df[fielding_df$teamID == team, "RF"]
  
  # Perform bootstrap sampling and compute the mean of resampled RF
  boot_samples <- replicate(1000, mean(sample(team_specific_rf, replace = TRUE), 
                                       na.rm = TRUE))
  
  boot_mat[, team] <- boot_samples
}


# Calculate standard deviation per team
std_dev_df <- as.data.frame(apply(boot_mat, 2, sd, na.rm = TRUE))
colnames(std_dev_df) <- c("std_dev")
std_dev_df$teamID <- rownames(std_dev_df)


end_time <- Sys.time()
non_parallel_time <- end_time - start_time
cat("Non-parallel execution time:", non_parallel_time, "\n")

##########################################################################################

# Timing parallel 
start_time <- Sys.time()

# Run the parallel bootstrap process using mclapply here
library(parallel)
set.seed(123, kind = "L'Ecuyer-CMRG")

num_cores <- detectCores() - 1

#' Use this function for the bootstrap sampling with replicate
bootstrap_function <- function(team) {
  # Get the RF values per team
  team_specific_rf <- fielding_df[fielding_df$teamID == team, "RF"]
  # 1000 bootstrap samples which calculate means 
  replicate(1000, mean(sample(team_specific_rf, replace = TRUE), na.rm = TRUE))
}

boot_mat_list <- mclapply(teams, bootstrap_function, mc.cores = num_cores)
boot_mat_parallel <- do.call(cbind, boot_mat_list)
colnames(boot_mat_parallel) <- teams

# Calculate standard deviation for the bootstrapping with parallel processing
std_dev_parallel_df <- as.data.frame(apply(boot_mat_parallel, 2, sd, na.rm = TRUE))
colnames(std_dev_parallel_df) <- c("std_dev_parallel")
std_dev_parallel_df$teamID <- rownames(std_dev_parallel_df)


end_time <- Sys.time()
parallel_time <- end_time - start_time
cat("Parallel execution time:", parallel_time, "\n")

##########################################################################################
# Timing parallel with future
start_time <- Sys.time()

# Run the parallel bootstrap process using future here
library(future)
options(future.rng.onMisuse = "ignore")
set.seed(123)
options(future.seed = TRUE)

plan(multisession, workers = detectCores() - 1) 

bootstrap_function <- function(team_data) {
  # 1000 bootstrap samples and the means calculated
  replicate(1000, mean(sample(team_data, replace = TRUE), na.rm = TRUE))
}

team_data_list <- lapply(teams, function(team) {
  fielding_df[fielding_df$teamID == team, "RF"]
})

# Make parallel jobs using futures
futures_list <- lapply(team_data_list, function(team_data) {
  future({
    bootstrap_function(team_data)
  })
})

# Get results using 506 notes
boot_mat_list <- lapply(futures_list, value)
boot_mat_future <- do.call(cbind, boot_mat_list)
colnames(boot_mat_future) <- teams

# Calculate standard deviation for the bootstrapping with future
std_dev_future_df <- as.data.frame(apply(boot_mat_future, 2, sd, na.rm = TRUE))
colnames(std_dev_future_df) <- c("std_dev_future")
std_dev_future_df$teamID <- rownames(std_dev_future_df)

end_time <- Sys.time()
future_time <- end_time - start_time
cat("Future execution time:", future_time, "\n")
```

```{r}
# Final Results
cat("Non-parallel execution time:", non_parallel_time, "\n")
cat("Parallel execution time:", parallel_time, "\n")
cat("Future execution time:", future_time, "\n")
```

We have that non-parallel execution time is higher than the other two, which we expect because the non-parallel execution method processes each team sequentially, performing bootstrap sampling and calculating standard deviations one at a time. This results in a longer execution time and becomes computationally inefficient as the number of teams or calculation complexity increases. In contrast, parallel execution using the mclapply function from the parallel package distributes the workload across multiple cores, significantly reducing the execution time. And lastly, the approach using the future package offers asynchronous execution, also reducing execution time but it also incurs overhead due to managing multiple jobs, making it a bit less efficient than the parallel processing method with mclapply for this task. Specifically, I am on a Mac and parallel on Unix systems is done using forking while on Windows, it is done using sockets, and sockets are slower. Since future is a socket based approach, it makes sense why it is less efficient on my system than parallel. Thus these results are reasonable in my case for non-parallel to be the slowest and the parallel method to be the fastest.

### Attribution of Sources

-   Chat GPT for "options(future.seed = TRUE)" and options(future.rng.onMisuse = "ignore") because I was getting warning messages for future
